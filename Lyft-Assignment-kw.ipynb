{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyft Data Science Assignment ---- kw\n",
    "\n",
    "First, I will conduct some Exploratory Data Analysis to better understand the churn at Lyft and main factors that affect a driver’s churn rate.\n",
    "\n",
    "Then, I will generate two different datasets to analyst by driver segment and some additional level.\n",
    "\n",
    "After, I will build a model to test the attributes I selected.\n",
    "\n",
    "In the end, I will give a summary and detail about experimentation design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for analysis and plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers = pd.read_csv('Resources/driver_ids.csv')\n",
    "rides_id = pd.read_csv('Resources/ride_ids.csv')\n",
    "rides_times = pd.read_csv('Resources/ride_timestamps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the first few rows\n",
    "drivers.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 937 unique driver ids with corresponding start dates for the drivers.\n",
    "There are no null values but the timestamp is a string, this will need to be converted to a datetime item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform timestamp to datetime\n",
    "drivers['driver_onboard_date'] = pd.to_datetime(drivers['driver_onboard_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the first few rows\n",
    "rides_id.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_id.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is data on 193502 different lyft journeys and there are no null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the first few rows\n",
    "rides_times.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_times.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataframe has the 194081 rows and there are no null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform timestamp to datetime\n",
    "rides_times['ride_picked_up_at'] = pd.to_datetime(rides_times['ride_picked_up_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features from drivers: extract the day, month and week of the year as well as month from the data frame\n",
    "drivers['driver_onboard_week'] = drivers['driver_onboard_date'].dt.weekofyear\n",
    "drivers['driver_onboard_month'] = drivers['driver_onboard_date'].dt.month\n",
    "drivers['driver_onboard_day'] = drivers['driver_onboard_date'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features from rides_timestamps\n",
    "rides_times['day_of_week'] = rides_times['ride_picked_up_at'].dt.weekday\n",
    "rides_times['month'] = rides_times['ride_picked_up_at'].dt.month\n",
    "rides_times['hour_of_day'] = rides_times['ride_picked_up_at'].dt.hour\n",
    "rides_times['week_of_year'] = rides_times['ride_picked_up_at'].dt.weekofyear\n",
    "rides_times['day_of_year'] = rides_times['ride_picked_up_at'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge rides_times with rides_id\n",
    "rides = pd.merge(rides_id, rides_times, on='ride_id', how='inner')\n",
    "# Next merge new dataframe with drivers\n",
    "rides = pd.merge(rides, drivers, on='driver_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the new dataset\n",
    "rides.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of data Fields\n",
    "driver_id - Unique identifier for a driver\n",
    "ride_id - Unique identifier for a ride that was completed by the driver\n",
    "ride_distance - Ride distance in meters\n",
    "ride_duration - Ride durations in seconds\n",
    "ride_prime_time - PrimeTime applied on the ride\n",
    "ride_picked_up_at - Timestamp for when driver picked up the passenger\n",
    "day_of_week - Day the ride took place\n",
    "month - Month the ride took place\n",
    "hour_of_day - Hour of the day the ride took place\n",
    "week_of_year - Week of the year ride took place\n",
    "day_of_year - Day of the year ride took place\n",
    "driver_onboard_date - Date on which driver was on-boarded\n",
    "driver_onboard_week - Week of the year on which driver was on-boarded\n",
    "driver_onboard_day - Day of the year on which driver was on-boarded\n",
    "driver_onboard_month - Month on which driver was on-boarded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explotatory data analysis\n",
    "To check the columns in the merged dataframe in order to check for outliers and get a better understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ride_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=rides['ride_distance'])\n",
    "sns.set_context('poster')\n",
    "sns.set(style='white')\n",
    "sns.despine(bottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove outliers: From the boxplot above we can see that the majority of the journeys are less than 40,000 meters in distance. There are a couple of outliers above this value as well as several shorter distances. I will remove rows where where ride_distance is less than 500 meters and greater than 40,000 metres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides = rides[(rides.ride_distance > 500) & (rides.ride_distance < 40000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(rides['ride_distance'], hist=True, kde=False)\n",
    "ax.set_title('Distribution of ride_distance\\n', size = 20)\n",
    "sns.set_context('poster')\n",
    "sns.set(style='white')\n",
    "sns.despine(bottom=False)\n",
    "ax.set_ylabel('Count', size = 14)\n",
    "ax.set_xlabel('Distance (m)', size = 14)\n",
    "ax.axvline(rides['ride_distance'].mean(), color = 'green', linewidth = 1)\n",
    "ax.axvline(rides['ride_distance'].median(), color = 'red', linewidth = 1)\n",
    "ax.legend(['Mean', 'Median'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above histogram we can see that the majority of journeys are less than 10,000 meters with the average distance being ~6,300 metres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ride_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides.ride_duration.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=rides['ride_duration'])\n",
    "sns.set_context('poster')\n",
    "sns.set(style='white')\n",
    "sns.despine(bottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove outliers: From the boxplot above we can see that the majority of the journeys are less than 3,000 seconds in duration. There are a couple of outliers above this value as well as several shorter distances. I will remove rows where where ride_duration is greater than 2,000 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides = rides[rides.ride_duration < 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(rides['ride_duration'], hist=True, kde=False)\n",
    "sns.set_context('poster')\n",
    "sns.set(style='white')\n",
    "sns.despine(bottom=False)\n",
    "ax.set_title('Distribution of ride duration\\n', size = 20)\n",
    "ax.set_ylabel('Count', size = 14)\n",
    "ax.set_xlabel('Duration (s)', size = 14)\n",
    "ax.axvline(rides['ride_duration'].mean(), color = 'green', linewidth = 1)\n",
    "ax.axvline(rides['ride_duration'].median(), color = 'red', linewidth = 1)\n",
    "ax.legend(['Mean', 'Median'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above histogram we can see that the majority of journeys take less than 2,000 seconds with the average distance being ~750 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ride_prime_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides.ride_prime_time.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(rides['ride_prime_time'], hist=True, kde=False)\n",
    "sns.set_context('poster')\n",
    "sns.set(style='white')\n",
    "sns.despine(bottom=False)\n",
    "ax.set_title('Distribution of PrimeTime\\n', size = 20)\n",
    "ax.set_ylabel('Count', size = 14)\n",
    "ax.set_xlabel('PrimeTime', size = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for the majority of journeys there is no PrimeTime pricing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### day_of_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.tsplot(rides.groupby('day_of_week')['day_of_week'].count())\n",
    "sns.set_context('poster')\n",
    "sns.set(style='white')\n",
    "sns.despine(bottom=False)\n",
    "ax.set_title('Distribution of journeys per day\\n', size = 20)\n",
    "ax.set_ylabel('Count', size = 14)\n",
    "ax.set_xlabel('Day of the week', size = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers of the week refer to the days of the week with 0 = Monday and 6 = Sunday.\n",
    "From the timeseries plot we can see that Monday has the least number of journeys and the number of journeys increase steadly during the week to peak on Fridays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hour_of_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.tsplot(rides.groupby('hour_of_day')['hour_of_day'].count())\n",
    "sns.set_context('poster')\n",
    "sns.set(style='white')\n",
    "sns.despine(bottom=False)\n",
    "ax.set_title('Distribution of journeys per hour of the day\\n', size = 20)\n",
    "ax.set_ylabel('Count', size = 14)\n",
    "ax.set_xlabel('Hour of the day', size = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### week_of_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(rides.week_of_year, bins=13, hist=True, kde=False)\n",
    "sns.set_context('poster')\n",
    "sns.set(style='white')\n",
    "sns.despine(bottom=False)\n",
    "ax.set_title('Distribution of journeys per week of the year\\n', size = 20)\n",
    "ax.set_ylabel('Count', size = 14)\n",
    "ax.set_xlabel('Week of the year', size = 14)\n",
    "ax.set_xticks(np.arange(13,26,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected we can see that that there is a gradual increase in the number journeys per week leading up to a peak in week 20, this is line with the previous distribution plot for months of the year where the peak was in May."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### day_of_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides.day_of_year.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(rides.day_of_year, bins=90, hist=True, kde=False)\n",
    "sns.set_context('poster')\n",
    "sns.set(style='white')\n",
    "sns.despine(bottom=False)\n",
    "ax.set_title('Distribution of journeys per day of the year\\n', size = 20)\n",
    "ax.set_ylabel('Count', size = 14)\n",
    "ax.set_xlabel('Day of the year', size = 14)\n",
    "ax.set_xticks(np.arange(88,180,7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is in line with the weeks of the year with the gradual increase in the number journeys per day leading up to a peak in day 135, this is line with the previous distribution plot for months of the year where the peak was in May."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### driver_onboard_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.tsplot(rides.groupby('day_of_year')['day_of_year'].count())\n",
    "ax = sns.distplot(rides.driver_onboard_week, bins=7, hist=True, kde=False)\n",
    "sns.set_context('poster')\n",
    "sns.set(style='white')\n",
    "sns.despine(bottom=False)\n",
    "ax.set_title('Distribution of driver onboarding per week of the year\\n', size = 20)\n",
    "ax.set_ylabel('Count', size = 14)\n",
    "ax.set_xlabel('Week of the year', size = 14)\n",
    "ax.set_xticks(np.arange(13,26,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above it can be seen that a large number of drivers came onboard earlier in the four month period and then decreased over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate output datasets ---- lifetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average lifetime of a driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defination of churn\n",
    "To define churn of activated drivers (a driver becomes ‘activated’ once they complete their first ride).\n",
    "I will assume that once a drivers doesn't drive for a week they have churned, so any driver that has not driven in the last week will be treated as churned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am looking for drivers that work that have consistant working hours that can meet demand at peak times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the start day and week that a driver starts and the last day they worked\n",
    "lifetime0 = pd.DataFrame(rides.pivot_table(index='driver_id', values=('day_of_year', 'week_of_year', \n",
    "                                                        'driver_onboard_day', 'driver_onboard_week'), aggfunc=np.max))\n",
    "lifetime0.fillna(0, inplace=True)\n",
    "lifetime0.reset_index( inplace=True)\n",
    "\n",
    "# Take the start day and week that a driver starts and the first day they completed the trip\n",
    "lifetime1 = pd.DataFrame(rides.pivot_table(index='driver_id', values=('day_of_year', 'week_of_year', \n",
    "                                                        'driver_onboard_day', 'driver_onboard_week'), aggfunc=np.min))\n",
    "lifetime1.fillna(0, inplace=True)\n",
    "lifetime1.reset_index( inplace=True)\n",
    "\n",
    "# Take the start day and week that a driver starts and the first day they completed the trip\n",
    "lifetime2 = pd.DataFrame(rides.pivot_table(index='driver_id', values=('ride_prime_time'), aggfunc=np.mean))\n",
    "lifetime2.fillna(0, inplace=True)\n",
    "lifetime2.reset_index( inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge\n",
    "lifetime3 = pd.merge(lifetime0, lifetime1,'left', on = 'driver_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge\n",
    "lifetime = pd.merge(lifetime3, lifetime2,'left', on = 'driver_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop useless columns\n",
    "lifetime=lifetime.drop('driver_onboard_day_y', axis=1)\n",
    "lifetime=lifetime.drop('driver_onboard_week_y', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetime = lifetime.rename(columns={\"driver_id\":\"driver_id\", \"day_of_year_x\": \"day_of_year\", \"driver_onboard_day_x\": \"driver_onboard_day\",\n",
    "                                                      \"driver_onboard_week_x\": \"driver_onboard_week\", \"week_of_year_x\": \"week_of_year\",\n",
    "                                                      \"day_of_year_y\": \"day_of_year_first\",\"week_of_year_y\": \"week_of_year_first\",\n",
    "                                                      \"ride_prime_time\": \"avg_ride_prime_time\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetime.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the amount of time a driver has worked\n",
    "lifetime['no_weeks'] = lifetime['week_of_year'] - lifetime['driver_onboard_week']\n",
    "lifetime['no_days'] = lifetime['day_of_year'] - lifetime['driver_onboard_day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(lifetime['no_days'], hist=True, kde=False, bins=30)\n",
    "sns.set_context('poster')\n",
    "sns.set(style='white')\n",
    "sns.despine(bottom=False)\n",
    "ax.set_title('Distribution of num of days working\\n', size = 20)\n",
    "ax.set_ylabel('Count', size = 14)\n",
    "ax.set_xlabel('Days', size = 14)\n",
    "ax.axvline(lifetime['no_days'].mean(), color = 'green', linewidth = 1)\n",
    "ax.axvline(lifetime['no_days'].median(), color = 'red', linewidth = 1)\n",
    "ax.legend(['Mean', 'Median'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the four months that the data was collected we can see that an average driver has worked for a period of 55 days.\n",
    "It is important to note that onboarding started earnestly in the first month and gradually increased as time progressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(lifetime['no_weeks'], hist=True, kde=False, bins=10)\n",
    "sns.set_context('poster')\n",
    "sns.set(style='white')\n",
    "sns.despine(bottom=False)\n",
    "ax.set_title('Distribution of no. of weeks working\\n', size = 20)\n",
    "ax.set_ylabel('Count', size = 14)\n",
    "ax.set_xlabel('Weeks', size = 14)\n",
    "ax.axvline(lifetime['no_weeks'].mean(), color = 'green', linewidth = 1)\n",
    "ax.axvline(lifetime['no_weeks'].median(), color = 'red', linewidth = 1)\n",
    "ax.legend(['Mean', 'Median'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echoing the above we can see that an average driver has worked for a period of ~7.5 weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the time spend a driver who completed first trip after onboard\n",
    "lifetime['no_weeks_first'] = lifetime['week_of_year_first'] - lifetime['driver_onboard_week']\n",
    "lifetime['no_days_first'] = lifetime['day_of_year_first'] - lifetime['driver_onboard_day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(lifetime['no_days_first'], hist=True, kde=False, bins=30)\n",
    "sns.set_context('poster')\n",
    "sns.set(style='white')\n",
    "sns.despine(bottom=False)\n",
    "ax.set_title('Distribution of num of days working\\n', size = 20)\n",
    "ax.set_ylabel('Count', size = 14)\n",
    "ax.set_xlabel('Days', size = 14)\n",
    "ax.axvline(lifetime['no_days_first'].mean(), color = 'green', linewidth = 1)\n",
    "ax.axvline(lifetime['no_days_first'].median(), color = 'red', linewidth = 1)\n",
    "ax.legend(['Mean', 'Median'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetime.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flag churn driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Any driver that has not driven in the last week will be treated as churned.\n",
    "churned = lifetime[lifetime['week_of_year'] < 25]\n",
    "print ('There are', churned.shape[0], 'churned drivers.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "323 churned drivers accounts for ~39% of the total drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column and add a binary indication of churn, 1 for churn and 0 otherwise\n",
    "\n",
    "lifetime['churn'] = 0\n",
    "\n",
    "for row in lifetime.index:\n",
    "    \n",
    "    if lifetime.loc[row,('week_of_year')]  < 25:\n",
    "    \n",
    "        lifetime.loc[row,('churn')] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetime.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new indication of driver_type\n",
    "    \n",
    "lifetime.loc[lifetime.no_days <= 30,'driver_type'] = 'new_driver'\n",
    "lifetime.loc[lifetime.no_days >= 60,'driver_type'] = 'driver_m60d'\n",
    "lifetime.loc[(lifetime.no_days < 60)&(lifetime.no_days > 30),'driver_type'] = 'driver_l60d'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = lifetime[lifetime['no_days'] <= 30]\n",
    "print ('There are', new.shape[0], 'new drivers.')\n",
    "\n",
    "lifetime.groupby(['driver_type','churn']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average hours worked per week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the start day and week that a driver starts and the last day they worked \n",
    "time_worked = pd.DataFrame(rides.pivot_table(index='driver_id', values=('ride_duration'), aggfunc=np.mean))\n",
    "time_worked.fillna(0, inplace=True)\n",
    "time_worked.reset_index( inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge\n",
    "lifetime = pd.merge(lifetime, time_worked,'left', on = 'driver_id')\n",
    "\n",
    "# Convert from seconds to hours\n",
    "lifetime['time_worked'] = lifetime.ride_duration/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetime.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(lifetime['time_worked'], hist=True, kde=False, bins=10)\n",
    "sns.set_context('poster')\n",
    "sns.set(style='white')\n",
    "sns.despine(bottom=False)\n",
    "ax.set_title('Distribution of hours worked\\n', size = 20)\n",
    "ax.set_ylabel('Count', size = 14)\n",
    "ax.set_xlabel('Hours', size = 14)\n",
    "ax.axvline(lifetime['time_worked'].mean(), color = 'green', linewidth = 1)\n",
    "ax.axvline(lifetime['time_worked'].median(), color = 'red', linewidth = 1)\n",
    "ax.legend(['Mean', 'Median'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This information only takes into account the amount of time a drives spends on paid journeys. The actual time the driver spends working is not represented.\n",
    "We can see a good portion of the drivers spent 10 hours or less driving passengers. The average number of hours spent driving was ~50 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of completed journeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the count of a drivers journeys over their lifetime\n",
    "no_rides = pd.DataFrame(rides.pivot_table(index='driver_id', values=('ride_id'), aggfunc='count'))\n",
    "no_rides.fillna(0, inplace=True)\n",
    "no_rides.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rides.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge\n",
    "lifetime = pd.merge(lifetime, no_rides,'left', on = 'driver_id')\n",
    "\n",
    "lifetime['no_rides'] = lifetime.ride_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop useless columns\n",
    "lifetime=lifetime.drop('ride_id', axis=1)\n",
    "lifetime=lifetime.drop('ride_duration', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetime.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(lifetime['no_rides'], hist=True, kde=False, bins=10)\n",
    "sns.set_context('poster')\n",
    "sns.set(style='white')\n",
    "sns.despine(bottom=False)\n",
    "ax.set_title('Distribution of no. of journeys\\n', size = 20)\n",
    "ax.set_ylabel('Count', size = 14)\n",
    "ax.set_xlabel('No. of journeys', size = 14)\n",
    "ax.axvline(lifetime['no_rides'].mean(), color = 'green', linewidth = 1)\n",
    "ax.axvline(lifetime['no_rides'].median(), color = 'red', linewidth = 1)\n",
    "ax.legend(['Mean', 'Median'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average number of journeys made by a driver in their lifetime is 206."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate output datasets ---- pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides['ride_picked_up_at'] = pd.to_datetime(rides['ride_picked_up_at'])\n",
    "rides['driver_onboard_date'] = pd.to_datetime(rides['driver_onboard_date'])\n",
    "rides['days_active'] = rides['ride_picked_up_at'] - rides['driver_onboard_date'] \n",
    "rides['days_active_in_days'] = rides['days_active']/ pd.Timedelta(days=1)\n",
    "rides['ride_within_1st_week'] = rides['days_active_in_days'] <= 7\n",
    "rides.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = rides.pivot_table(index='driver_id', \\\n",
    "                          values=('days_active_in_days', 'ride_id', 'ride_prime_time','ride_picked_up_at','ride_within_1st_week'),\\\n",
    "                          aggfunc={'days_active_in_days':np.max,'ride_id': 'count', 'ride_prime_time':np.mean,\\\n",
    "                                   'ride_picked_up_at': min,'ride_within_1st_week': sum})\\\n",
    "                                 .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot['days_active'] = pivot['days_active_in_days']\n",
    "pivot['number_rides'] = pivot['ride_id']\n",
    "#drop useless columns\n",
    "pivot=pivot.drop('ride_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Churn rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_churn = pd.merge(drivers, pivot, how = 'left', on='driver_id')\n",
    "df_churn = df_churn[['driver_id', 'driver_onboard_date', 'days_active', 'ride_picked_up_at']]\n",
    "\n",
    "retention_rates_by_day = []\n",
    "\n",
    "for i in range(43):\n",
    "    retention = (df_churn['days_active'] >= i).mean()\n",
    "    retention_rates_by_day.append(retention)\n",
    "    \n",
    "plt.plot(retention_rates_by_day, color='fuchsia')\n",
    "\n",
    "plt.title('Drivers churn rate', fontsize = 16)\n",
    "\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Retention')\n",
    "sns.despine(offset=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg number of rides 1st week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 5))\n",
    "x = [11.016018,36.058000]\n",
    "plt.bar(x = ('<150 rides', '>150 rides'), height = x, align='center', color=('gray', 'fuchsia'))\n",
    "plt.ylabel('Number of rides 1st week')\n",
    "plt.title('Avg number of rides 1st week')\n",
    "sns.despine(offset=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg number of trips/day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 5))\n",
    "y = [1.742499,5.353718]\n",
    "plt.bar(x = ('<150 rides', '>150 rides'), height = y, align='center',  color=('gray', 'fuchsia'))\n",
    "plt.ylabel('Avg number trips/day')\n",
    "plt.title('Avg number of trips/day')\n",
    "sns.despine(offset=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Driver tenure histogram, days', fontsize = 16)\n",
    "pivot['days_active'].hist(bins = 50, facecolor='fuchsia')\n",
    "plt.xlabel('Number days active')\n",
    "plt.grid(False)\n",
    "sns.despine(offset=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of rides per driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Number of rides per driver', fontsize = 16)\n",
    "pivot['number_rides'].hist(bins = 50, facecolor='fuchsia')\n",
    "plt.xlabel('Number of rides')\n",
    "plt.ylabel('Number of drivers')\n",
    "plt.grid(False)\n",
    "sns.despine(offset=10, trim=True)\n",
    "#plt.xlim((0, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Driver tenure histogram (by cohort), days', fontsize = 14)\n",
    "x = pivot.loc[(pivot['days_active'].notnull()) & (pivot['number_rides'] < 150), 'days_active']\n",
    "y = pivot.loc[(pivot['days_active'].notnull()) & (pivot['number_rides'] >= 150), 'days_active']\n",
    "plt.hist([x, y], color=('gray', 'fuchsia'), label = ('< 150','>= 150'))\n",
    "sns.despine(offset=10)\n",
    "plt.xlabel('Number of days active')\n",
    "plt.ylabel('Number of drivers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of rides per tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pivot['number_rides'], pivot['days_active'], alpha=0.3, color = 'fuchsia') \n",
    "plt.xlabel('Number of rides')\n",
    "plt.ylabel('Driver tenure, days')\n",
    "plt.title('Number of rides per tenure', fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of rides in the first week after onboarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot['ride_within_1st_week'].hist( color='fuchsia')\n",
    "plt.title('Number of rides in the first week after onboarding', fontsize = 16)\n",
    "\n",
    "plt.xlabel('Number of rides')\n",
    "\n",
    "sns.despine(offset=10)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotheses-1\n",
    "Use pivot as dataset to analysis whether \"Doubling the number of rides in an activated driver’s first week\"-----\"pivot.ride_within_1st_week\" feature will decrease driver churn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotheses-2\n",
    "Use lifetime as dataset to analysis whether \"Increased the amount of time a drives spends on paid journeys\"(Methods: promotion/subsidize)-----\"lifetime.time_worked\" feature will decrease driver churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion base on above data analyst\n",
    "I have more confidence about second hypothese. Not only from above results, also after I build a logic-regression model, I confirm my hypothese-2 have a stronger affect on drive churn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "Use lifetime as dataset to predict whether the customer will churn or not. To better understand which factor will strongly affect the chrun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetime.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for modelling\n",
    "from sklearn import datasets, metrics, linear_model, feature_selection, preprocessing\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use Logistic Regression to predict churn determine.\n",
    "\n",
    "The target will be the churn variable I created previously.\n",
    "\n",
    "The feature set will include:\n",
    "\n",
    "1)no_days: the amount of time a driver has worked\n",
    "2)no_days_first: the time spend a driver who completed first trip after onboard\n",
    "3)time_worked: the amount of time a drives spends on paid journeys\n",
    "4)no_rides: the count of a drivers journeys over their lifetime\n",
    "5)avg_ride_prime_time: the avg PrimeTime applied on the ride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and feature set for modelling\n",
    "X = lifetime[['no_days', 'no_days_first','time_worked', 'no_rides', 'avg_ride_prime_time']]\n",
    "y = lifetime.churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design an experiment. Standardise the features before splitting into training and test sets\n",
    "\n",
    "# Define the scaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Scaling the feature set\n",
    "Xs = ss.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardisation is necessary for regularized regression because the beta values for each predictor variable must be on the same scale. If betas are different sizes just because of the scale of predictor variables the regularisation term can't determine which betas are more/less important based on their size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our model\n",
    "# Using the standard paramaeters of the model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fitting the model with our target and features\n",
    "model = lr.fit(Xs, y)\n",
    "# Determins preditions from the model and print the accuracy of the model\n",
    "predictions = model.predict(Xs)\n",
    "print ('Accuracy:', accuracy_score(y, predictions))\n",
    "\n",
    "# Setting the feature importance to a variable\n",
    "feat_importance = model.coef_\n",
    "# Sorting the feature importance\n",
    "indices = np.argsort(np.absolute(feat_importance))\n",
    "\n",
    "# Plotting the feature importance\n",
    "plt.figure(figsize = (16, 6))\n",
    "plt.title(\"Feature importance\\n\", fontsize = 20)\n",
    "plt.bar(range(Xs.shape[1]), feat_importance[0][indices][0][::-1], align=\"center\")\n",
    "plt.ylabel('Feature coefficient', size = 16)\n",
    "plt.xlabel('Feature variables', size = 16)\n",
    "plt.xticks(range(X.shape[1]), X.columns[indices][0][::-1], rotation=45)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to display the odds ratio\n",
    "odds_ratio = pd.DataFrame(np.transpose(model.coef_), index = X.columns, columns = [\"Coeffecient value\"])\n",
    "# Create the odds colums by calculating the exponential of the coefficients\n",
    "odds_ratio[\"Odds\"] = odds_ratio[\"Coeffecient value\"].apply(np.exp)\n",
    "# Display the odds ratio\n",
    "odds_ratio.sort_values(by = 'Odds', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above chart and table we can see time_worked, avg_ride_prime_time and no_days_first come out on top for predicting churn with an Accuracy: 0.8399044205495818\n",
    "\n",
    "Cross-validation\n",
    "Using cross-validation I will evaluate different metrics -\n",
    "\n",
    "Accuracy = (TP+TN)/total\n",
    "Precision = TP/(TP+FP)\n",
    "Recall = TP/(TP+FN)\n",
    "I will also plot a confusion matrix to understand these metrics better as well as a receiver operating characteristic (ROC) curve to look at the area under the curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the metrics for evaluation\n",
    "metrics = ['accuracy', 'precision', 'recall']\n",
    "\n",
    "# Define the model \n",
    "# Using L2 regularisation and 5-fold cross-validation\n",
    "lg = LogisticRegressionCV(penalty = 'l2', cv = 5, solver = 'liblinear') \n",
    "\n",
    "# Loop through the different metrics and print the mean of the metric after the 5-fold cross-validation\n",
    "for metric in metrics:\n",
    "    \n",
    "    scores = cross_val_score(lg, Xs, y, scoring = metric, cv = 5)\n",
    "    \n",
    "    print (metric, ':' , scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confucion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for plotting confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix for high/low salaries\\n', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True salary')\n",
    "    plt.xlabel('Predicted salary')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cnf_matrix = confusion_matrix(y, model.predict(Xs))\n",
    "plot_confusion_matrix(cnf_matrix, classes= ['Low', 'High'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "FPR, TPR, THR = roc_curve(y, model.predict_proba(Xs)[:,1])\n",
    "ROC_AUC = auc(FPR, TPR)\n",
    "\n",
    "plt.figure(figsize=[6,6])\n",
    "plt.plot(FPR, TPR, label='ROC curve (area = %0.2f)' % ROC_AUC, linewidth = 4)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate ', fontsize=16)\n",
    "plt.title('ROC curve \\n', fontsize=20)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy answers the question - Overall, how often is the classifier correct?\n",
    "\n",
    "- For this model the accuracy score was 0.83 which is a good accuracy for the model and above the baseline. This means that model is good at predicting churn over random choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch\n",
    "To try and improve the model I will perform a GridSearch using different parameters to try and report the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GridSearch parameters\n",
    "params = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "          'penalty': ['l1','l2']\n",
    "         }\n",
    "\n",
    "# Define the model\n",
    "grid = GridSearchCV(LogisticRegression(), params, cv=5)\n",
    "# fit the model\n",
    "grid.fit(Xs, y)\n",
    "# Print the beat parameters to use\n",
    "print (grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'penalty': 'l2', 'C': 0.01}\n",
    "The best parameters identified by the GridSearch were -\n",
    "\n",
    "penalty of l2 (Ridge)\n",
    "C value of 0.01\n",
    "I will run another regression with these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using L2 regularisation, C = 1 and solver = liblinear as this is better for smaller datasets\n",
    "lggs = LogisticRegression(penalty = 'l2', C = 0.01, solver = 'liblinear')\n",
    "\n",
    "# Fit the model with our target and features\n",
    "lggs.fit(Xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the metrics for evaluation again\n",
    "metrics = ['accuracy', 'precision', 'recall']\n",
    "\n",
    "# Loop through the different metrics and print the mean of the metric after the 5-fold cross-validation\n",
    "for metric in metrics:\n",
    "    \n",
    "    scores = cross_val_score(lggs, Xs, y, scoring = metric, cv = 5)\n",
    "    \n",
    "    print (metric, ':' , scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cnf_matrix = confusion_matrix(y, lggs.predict(Xs))\n",
    "plot_confusion_matrix(cnf_matrix, classes= ['Low', 'High'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "FPR, TPR, THR = roc_curve(y, lggs.predict_proba(Xs)[:,1])\n",
    "ROC_AUC = auc(FPR, TPR)\n",
    "\n",
    "plt.figure(figsize=[6,6])\n",
    "plt.plot(FPR, TPR, label='ROC curve (area = %0.2f)' % ROC_AUC, linewidth = 4)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate ', fontsize=16)\n",
    "plt.title('ROC curve \\n', fontsize=20)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analyst:\n",
    "There wasn't a noticable increase in the metric scores after using the optimised parameters identified from the GridSearch. This means that the original model was optimised to begin with.\n",
    "\n",
    "By analysing the datasets we were able to derive a lot of useful information about driver behaviour.\n",
    "\n",
    "Additional insights were obtained by performing some feature engineering on the datasets and these were used to identify factors that contribute to a driver's Lifetime Value.\n",
    "\n",
    "1)An optimised Logistic Regression model was used on the following features:\n",
    "\n",
    "no_days\n",
    "no_days_first\n",
    "time_worked\n",
    "no_rides\n",
    "avg_ride_prime_time\n",
    "\n",
    "2)With an accuracy score od 0.84 the following deatures:\n",
    "\n",
    "time_worked\n",
    "avg_ride_prime_time\n",
    "no_days_first\n",
    "Emmerged as being the best predisctors for identifying churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "● The definition (with justification) for a driver to be considered churned.\n",
    "Any driver that has not driven in the last week will be treated as churned.\n",
    "\n",
    "● An assessment on the current business impact of churn to Lyft.\n",
    "Find out the Reasons for churn we will better keep our users and decrease the churn rate.\n",
    "Anslyst by driver segemnts we can know:\n",
    "\n",
    "1)Drivers try the service and realize it's not something they like doing.\n",
    "Improve: customer service/survey analyst\n",
    "2)New customer mostly like to churn.\n",
    "Improve: onboarding system/give driver additional motivation\n",
    "\n",
    "● Insights on factors affecting churn.\n",
    "From above analyst and modeling, we got below factors affecting churn.\n",
    "time_worked\n",
    "avg_ride_prime_time\n",
    "no_days_first\n",
    "\n",
    "● Insights on segments of drivers more likely to churn.\n",
    "Explotatory data analysis. \n",
    "1)Pivot table:\n",
    "    Driver segments:\n",
    "    drivers with total rides<150:active on avg 40 days, make on average 53 rides\n",
    "    drivers with total rides>150:active on avg 66 days, make on average 340 rides\n",
    "    \n",
    "2)Lifetime table:\n",
    "    Driver segments:\n",
    "    new_driver: total work days < 30 after onboard date, as new driver. High percentage to churn.\n",
    "    driver_l60d: between 30~60 days, as driver less than 60 days.\n",
    "    driver_m60d: more than 60 days, as driver more than 60 days. Low percentage to churn.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "\n",
    "For test of hypothesis: “eliminating the Prime Time feature will decrease driver churn”.\n",
    "\n",
    "● What are the primary and secondary metrics you will track.\n",
    "\n",
    "Set up Significance Levels (Alpha) = 0.05\n",
    "primary metric: estimate_arrive_time in prime time\n",
    "\n",
    "secondary metrics: \n",
    "1)ride_duration in prime time \n",
    "2)ride_cancellation_rate in prime time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power analyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "● How long you will run the experiment and how you will choose the winning variant.\n",
    "Before running the A/B test,run the power analyst to determine the sample size. \n",
    "Then to know how long we need to run to reach the requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "● How you will divide observational units into control and treatment, and a description of the treatment and control conditions.\n",
    "\n",
    "1)After the power analyst,we need to randomly assign users to either the treatment or control group, for instance by hashing their user IDs into buckets. Randomly select samples from each segments of drivers to avoid bias.\n",
    "To distributing co-variates evenly eliminating statistical bias:\n",
    "    i) Randamization bias\n",
    "    ii) Selection bias, avoid risk appetite effect\n",
    "\n",
    "2)To estimate the effect of the treatment on a metric of interest for a random-user experiment:\n",
    "\n",
    "Estimate the control values by restricting to users in the control group\n",
    "Estimate the treatment values by restricting to users in the treatment group\n",
    "Then to compute the relative difference between the estimates from control & treatment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality assumptions\n",
    "Chek data normality,and check R-square between the relationship. Dealing with skewed/not skewed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis testing\n",
    "The hypothesis test will yield a p-value, which is the probability that our data could generate purely by chance.\n",
    "1)Variance and standard deviation\n",
    "2)P-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business impacts\n",
    "● What are some potential second-order effects on the experience of drivers and passengers during this experiment.\n",
    "\n",
    "When there is no Prime Time, a passenger who opens the app and sees a driver available always requests a ride.\n",
    "When there is Prime Time, the same passenger has a 50% chance of requesting a ride.\n",
    "Neither drivers nor passengers ever cancel — every request leads to a completed ride.\n",
    "\n",
    "If we eliminating the Prime Time,\n",
    "1)for drivers: \n",
    "it will get hard to maintain driver availability. Driver revenue/day decrease. Driver churn rate increase.\n",
    "\n",
    "2)for passengers: \n",
    "1)passengers waiting time may increased \n",
    "2)passengers cancellation increased \n",
    "3)passengers (users) churn rate increase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
